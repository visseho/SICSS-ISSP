
---
title: "Solutions des exercices Machine Learning - Kaggle Home Value"
author: "Ton Nom"
date: "`r format(Sys.Date(), '%d %B %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(caret)
library(glmnet)
library(randomForest)
```

# ðŸ“˜ Chargement des donnÃ©es

```{r}
# Exemple de lecture (remplace par ton chemin)
data <- read_csv("home_value_dataset.csv")
head(data)
```

---

# ðŸŸ¢ Exercice 1 â€“ RÃ©sumÃ© statistique

```{r}
summary(data)
sapply(data, sd)
```

---

# ðŸŸ¢ Exercice 2 â€“ Visualisations exploratoires

```{r}
# Histogramme
ggplot(data, aes(House_Price)) + geom_histogram(bins=30, fill="steelblue")

# Scatter plot
ggplot(data, aes(Square_Footage, House_Price)) + geom_point()

# CorrÃ©lation
cor(data %>% select(-Year_Built))
```

---

# ðŸŸ¡ Exercice 3 â€“ RÃ©gression linÃ©aire simple

```{r}
lm_simple <- lm(House_Price ~ Square_Footage, data=data)
summary(lm_simple)

# PrÃ©diction
predict(lm_simple, newdata=data.frame(Square_Footage=2000))
```

---

# ðŸŸ¡ Exercice 4 â€“ RÃ©gression linÃ©aire multiple

```{r}
lm_multi <- lm(House_Price ~ ., data=data)
summary(lm_multi)
```

---

# ðŸŸ¡ Exercice 5 â€“ Validation croisÃ©e

```{r}
set.seed(123)
trainIndex <- createDataPartition(data$House_Price, p=0.7, list=FALSE)
trainData <- data[trainIndex, ]
testData <- data[-trainIndex, ]

lm_cv <- lm(House_Price ~ ., data=trainData)
pred_train <- predict(lm_cv, trainData)
pred_test <- predict(lm_cv, testData)

# RMSE
sqrt(mean((trainData$House_Price - pred_train)^2))
sqrt(mean((testData$House_Price - pred_test)^2))
```

---

# ðŸŸ¢ Exercice 6 â€“ Transformation des variables

```{r}
data <- data %>% mutate(Age=2025 - Year_Built)
data <- data %>% mutate(Log_Lot_Size=log1p(Lot_Size))

lm_transformed <- lm(House_Price ~ Square_Footage + Num_Bedrooms + Num_Bathrooms +
                     Age + Log_Lot_Size + Garage_Size + Neighborhood_Quality, data=data)
summary(lm_transformed)
```

---

# ðŸŸ¢ Exercice 7 â€“ Ridge et Lasso

```{r}
x <- model.matrix(House_Price ~ . -1, data)
y <- data$House_Price

# Ridge
ridge_model <- cv.glmnet(x, y, alpha=0)
plot(ridge_model)
coef(ridge_model, s="lambda.min")

# Lasso
lasso_model <- cv.glmnet(x, y, alpha=1)
plot(lasso_model)
coef(lasso_model, s="lambda.min")
```

---

# ðŸŸ¢ Exercice 8 â€“ Random Forest Regression

```{r}
set.seed(123)
rf_model <- randomForest(House_Price ~ ., data=data, ntree=100, importance=TRUE)
rf_model
varImpPlot(rf_model)
```

---

# ðŸŸ¡ Exercice 9 â€“ Simulation et prÃ©diction

```{r}
new_house <- data.frame(
  Square_Footage=2500,
  Num_Bedrooms=4,
  Num_Bathrooms=3,
  Year_Built=2010,
  Lot_Size=0.5,
  Garage_Size=2,
  Neighborhood_Quality=8
)
new_house$Age <- 2025 - new_house$Year_Built
new_house$Log_Lot_Size <- log1p(new_house$Lot_Size)

predict(lm_multi, newdata=new_house)
predict(rf_model, newdata=new_house)
```

---

# ðŸŸ¢ Exercice 10 â€“ InterprÃ©tation des coefficients

- Coefficient positif sur `Neighborhood_Quality`: chaque point de qualitÃ© supplÃ©mentaire augmente le prix attendu.
- Coefficient proche de zÃ©ro dans Lasso: variable peu informative, retirÃ©e par pÃ©nalisation.

---

# ðŸŸ¢ Exercice 11 â€“ Feature Engineering

```{r}
data <- data %>% mutate(Interaction=Square_Footage * Neighborhood_Quality)
lm_interaction <- lm(House_Price ~ . + Interaction, data=data)
summary(lm_interaction)
```

---

# ðŸŸ¢ Exercice 12 â€“ Analyse des rÃ©sidus

```{r}
residuals <- resid(lm_multi)
hist(residuals)
plot(predict(lm_multi), residuals)
```

---

# ðŸŒŸ Projet final

- Nettoyage, feature engineering, entraÃ®nement, Ã©valuation, visualisation des erreurs.

Fin du document.
