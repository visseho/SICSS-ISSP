library(tidyverse)
# load cleaned data file for survey results
data <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2020_06_clean_mturk_data.csv")
# load pew benchmarks
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
# Charger les "vrais" résultats
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
pew <- pew %>% select(qid, pew_estimate)
# Effacer l'environnement
rm(list = ls())
# Charger les packages
library(tidyverse)
library(lme4)
# Charger les données appurées
data <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2020_06_clean_mturk_data.csv")
# Charger les informations additionnelles -- Les données sur la population
census <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2017_acs_data_clean.csv")
# Charger les "vrais" résultats
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
pew <- pew %>% select(qid, pew_estimate)
# take the mean of survey responses in mturk data
## remove demographic variables (factor vars)
## get column means
mturk_means <- data %>% select(-sex, -race, -age_cat, -region, -educ) %>%
summarise_all(~mean(., na.rm = T)) %>%
## reshape from wide to long using tidyr package
## with columns for questions (call this qid) and for mean
## having the data in long format makes it easier to merge with the pew estimate for plotting figure 1
pivot_longer(COVIDNEWSSWITCH.1:ALG_JOBCANDIDATE.1, names_to = "qid",
values_to = "mean")
# preview
head(mturk_means)
# merge mturk mean estimates with benchmark
mean_est <- inner_join(pew, mturk_means, by = c("qid"))
head(mean_est)
# make function for Figure 1
plot_comparison <- function(est_table, method, caption){
graph <-  ggplot(est_table,
aes(x = pew_estimate, y = method)) +
geom_point() +
labs(x = "Estimates from Pew", y = caption) +
scale_x_continuous(limits = c(0,1)) +
scale_y_continuous(limits = c(0,1)) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
coord_fixed()
return(graph)
}
# plot
plot_comparison(est_table = mean_est,
method = mean_est$mean,
caption = "Non-weighted estimates from MTurk")
# calculate difference
mean_est$diff <- abs(mean_est$mean - mean_est$pew_estimate)
# function for plotting difference
plot_diff <- function(est_table){
diff_graph <- ggplot(est_table, aes(x = diff)) +
geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = .025,
colour = "black", fill = "white") +
theme_bw() +
geom_vline(aes(xintercept = median(diff)), linetype = "longdash") +
labs(x = "absolute difference", y = "density") +
scale_y_continuous(limits = c(0, 0.45))
return(diff_graph)
}
# plot
plot_diff(mean_est)
#install.packages("tidytext")
#install.packages("textdata")
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:8])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 3, control = list(seed = 3425))
View(trump_tweet_lda)
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
# mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
#Adjust your working directory here
knitr::opts_knit$set(root.dir = '/Volumes/Workspace/OneDrive - Universite de Montreal/Canadian_Newspapers/Canadian newspaper')
rm(list = ls())
# Voici la liste des packages utilisés pour cet article
library('data.table')
library('tidytext')
library('tidyverse')
library('lubridate')
library('furrr')
library('future')
library('prettydoc')
install.packages("prettydoc")
df1 <- read_csv("newspapers.csv")
df1 <- read_csv("newspapers.csv")
#Adjust your working directory here
knitr::opts_knit$set(root.dir = '/Volumes/Workspace/OneDrive - Universite de Montreal/Canadian_Newspapers/Canadian newspaper')
df1 <- read_csv("newspapers.csv")
library(tidyverse)
library(tidyverse)
library(tidytext)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
View(trumptweets)
trumptweets$text[2]
trump_corpus <- Corpus(VectorSource(as.vector(trumptweets$text)))
trump_corpus
View((trump_corpus))
View(trump_corpus)
# Les informations de chaque document
trump_corpus[[1]][["content"]]
trump_corpus[[1]][["meta"]]
trump_corpus <- tm_map(trump_corpus, content_transformer(removeNumbers))
# Transformer en minuscule
trump_corpus <- tm_map(trump_corpus,  content_transformer(tolower))
# Enlever les espaces
trump_corpus <- tm_map(trump_corpus, content_transformer(stripWhitespace))
# Stemming
trump_corpus  <- tm_map(trump_corpus, content_transformer(stemDocument), language = "english")
trump_DTM <- DocumentTermMatrix(trump_corpus, control = list(wordLengths = c(5, Inf)))
View(trump_DTM)
inspect(trump_DTM[1:6,1:10])
trump_DTM_matrix[25:50,12:17]
inspect(trump_DTM[12:16,21:30])
trump_DTM_matrix <- as.matrix(trump_DTM)
trump_DTM_matrix[1:5,1:8]
trump_DTM_matrix[10:15,1:8]
View(trumptweets)
View(trumptweets)
tidy_trump_tweets <-
trumptweets[1:3, ] %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
View(tidy_trump_tweets)
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
View(tidy_trump_tweets)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
data("stop_words")
head(stopwords(), 24)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
http_trump <- data.frame(word = c("https", "t.co", "amp", "rt"))
http_trump
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(http_trump)
tidy_trump_tweets  %>%
count(word) %>%
arrange(desc(n))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets <- tidy_trump_tweets %>%
mutate_at("word", funs(wordStem((.), language="en")))
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
# Document-term matrix
tidy_trump_DTM <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(tidy_trump_DTM[1:5, 1:8])
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
top_20 <- top_20[1:20, ]
View(top_20)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 70, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE)
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE) +
coord_flip()
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
# theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE) +
coord_flip()
ggplot(top_20) +
geom_col(aes(x = word, y = n, fill = word)) +
theme_bw() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
ylab("Number of time a word appears in a tweet") +
xlab("word") +
guides(fill = FALSE) #+
View(tidy_trump_tweets)
tidy_trump_tweets_tfidf <-
tidy_trump_tweets %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
View(tidy_trump_tweets_tfidf)
top_tfidf <-
tidy_trump_tweets_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
tidy_trump_tfidf<- trumptweets %>%
select(created_at,text) %>%
unnest_tokens("word", text) %>%
anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n) %>%
filter(tf_idf < 4)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(tf_idf))
top_tfidf
top_tfidf$word[1]
dtm_trumptweets <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
dtm_trumptweets_matrix <- as.matrix(dtm_trumptweets)
dtm_trumptweets_matrix %>% {
wordcloud(.$word, .$n, max.words = 20)
}
rm(list = ls())
library(tidyverse)
library(tidytext)
library(textdata) #
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
trumptweets <- readRDS("../Données/trumptweets.RData")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)     # Tokenise the data
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("https|t.co|amp|rt", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
filter(!grepl("\\b\\d+\\b", word))
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(word = gsub("\\s+","", word))
tidy_trump_tweets %>%
count(word, sort = TRUE)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
top_20 <-
tidy_trump_tweets %>%
count(word, sort = TRUE)
top_20 <- top_20[1:20, ]
ggplot(top_20) +
geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
theme_minimal() +
theme(axis.text = element_text(angle = 90, hjust = 1)) +
labs(x = "mot", y = "Nombre de fois que le mot apparait dans un tweet") +
guides(fill = FALSE)
tidy_trump_tfidf <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text) %>%
# anti_join(stop_words) %>%
count(word, created_at) %>%
bind_tf_idf(word, created_at, n)
top_tfidf <- tidy_trump_tfidf %>%
arrange(desc(n))
top_tfidf <-
tidy_trump_tfidf %>%
arrange(desc(tf_idf))
economic_dictionary <- "economy|unemployment|trade|tariffs|employment|tax"
economic_dictionary1 <- "Togo|Cameroon"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
222/3196*100
economic_dictionary1 <- "Togo|Cameroon|Burkina"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary1))
economic_dictionary <- "economy|unemployment|trade|tariffs|employment|tax"
economic_tweets <-
trumptweets %>%
filter(str_detect(text, economic_dictionary))
head(economic_tweets$text)
head(get_sentiments("afinn"), 24)
head(get_sentiments("bing"), 24)
head(get_sentiments("nrc"), 24)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
dictionnaire <- get_sentiments("bing")
View(dictionnaire)
trump_tweet_sentiment <-
tidy_trump_tweets %>%
inner_join(dictionnaire) %>%
count(created_at, sentiment)
View(trump_tweet_sentiment)
ggplot(trump_tweet_sentiment) +
geom_line(aes(x=created_at, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
View(trumptweets)
tidy_trump_tweets <-
tidy_trump_tweets %>%
mutate(date = date(created_at))
View(tidy_trump_tweets)
trump_tweet_sentiment1 <-
tidy_trump_tweets %>%
inner_join(get_sentiments("bing")) %>%
count(date, sentiment)
ggplot(trump_tweet_sentiment1) +
geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw()
ggplot(trump_tweet_sentiment1) +
geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
theme_minimal() +
labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
theme_bw() +
facet_wrap(~sentiment)
#install.packages("tidytext")
#install.packages("textdata")
rm(list = ls())
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
#load(url("https://cbail.github.io/Trump_Tweets.Rdata"))
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
View(tidy_trump_tweets)
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:6])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 3, control = list(seed = 123))
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
tt_top_term
tt_top_term
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 4, control = list(seed = 123))
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
k <- c(2:5)
rm(list = ls())
rm(list = ls())
library(tidyverse)
library(lubridate)
library(stringr)
library(forcats)
library(modelr)
library(tm)
library(SnowballC)
library(tidytext)
library(wordcloud)
library(qss)
install.packages("qss")
library(qss)
# if you have not installed the `devtools` package
# install.packages("devtools")
library("devtools")
# if you have not installed the `devtools` package
install.packages("devtools")
library("devtools")
install_github("kosukeimai/qss-package", build_vignettes = TRUE)
library(qss)
DIR_SOURCE <- system.file("extdata/federalist", package = "qss")
corpus_raw <- VCorpus(DirSource(directory = DIR_SOURCE, pattern = "fp"))
corpus_raw
View(corpus_raw)
