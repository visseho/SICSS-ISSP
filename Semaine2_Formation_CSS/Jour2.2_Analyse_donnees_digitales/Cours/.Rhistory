library(tidyverse)
# load cleaned data file for survey results
data <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2020_06_clean_mturk_data.csv")
# load pew benchmarks
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
# Charger les "vrais" résultats
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
pew <- pew %>% select(qid, pew_estimate)
# Effacer l'environnement
rm(list = ls())
# Charger les packages
library(tidyverse)
library(lme4)
# Charger les données appurées
data <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2020_06_clean_mturk_data.csv")
# Charger les informations additionnelles -- Les données sur la population
census <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/2017_acs_data_clean.csv")
# Charger les "vrais" résultats
pew <- read_csv("https://raw.githubusercontent.com/compsocialscience/summer-institute/master/2020/materials/day4-surveys/activity/pew_benchmark_question_source_sicss_2020.csv")
pew <- pew %>% select(qid, pew_estimate)
# take the mean of survey responses in mturk data
## remove demographic variables (factor vars)
## get column means
mturk_means <- data %>% select(-sex, -race, -age_cat, -region, -educ) %>%
summarise_all(~mean(., na.rm = T)) %>%
## reshape from wide to long using tidyr package
## with columns for questions (call this qid) and for mean
## having the data in long format makes it easier to merge with the pew estimate for plotting figure 1
pivot_longer(COVIDNEWSSWITCH.1:ALG_JOBCANDIDATE.1, names_to = "qid",
values_to = "mean")
# preview
head(mturk_means)
# merge mturk mean estimates with benchmark
mean_est <- inner_join(pew, mturk_means, by = c("qid"))
head(mean_est)
# make function for Figure 1
plot_comparison <- function(est_table, method, caption){
graph <-  ggplot(est_table,
aes(x = pew_estimate, y = method)) +
geom_point() +
labs(x = "Estimates from Pew", y = caption) +
scale_x_continuous(limits = c(0,1)) +
scale_y_continuous(limits = c(0,1)) +
geom_abline(intercept = 0, slope = 1, linetype = "dotted") +
coord_fixed()
return(graph)
}
# plot
plot_comparison(est_table = mean_est,
method = mean_est$mean,
caption = "Non-weighted estimates from MTurk")
# calculate difference
mean_est$diff <- abs(mean_est$mean - mean_est$pew_estimate)
# function for plotting difference
plot_diff <- function(est_table){
diff_graph <- ggplot(est_table, aes(x = diff)) +
geom_histogram(aes(y = (..count..)/sum(..count..)), binwidth = .025,
colour = "black", fill = "white") +
theme_bw() +
geom_vline(aes(xintercept = median(diff)), linetype = "longdash") +
labs(x = "absolute difference", y = "density") +
scale_y_continuous(limits = c(0, 0.45))
return(diff_graph)
}
# plot
plot_diff(mean_est)
#install.packages("tidytext")
#install.packages("textdata")
library(tidyverse)
library(tidytext)
library(textdata)
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)
library(topicmodels)
trumptweets <- readRDS("../Données/trumptweets.Rdata")
tidy_trump_tweets <-
trumptweets %>%
select(created_at, text) %>%
unnest_tokens("word", text)
tidy_trump_tweets <-
tidy_trump_tweets %>%
anti_join(stop_words)
tidy_trump_tweets <-
tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ]
tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]
tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)
tidy_trump_tweets %>%
count(word) %>%
arrange(desc(n))
trump_tweets_dtm <-
tidy_trump_tweets %>%
count(created_at, word) %>%
cast_dtm(created_at, word, n)
inspect(trump_tweets_dtm[1:5,1:8])
trump_tweet_lda <- LDA(trump_tweets_dtm, k = 3, control = list(seed = 3425))
View(trump_tweet_lda)
trump_tweet_lda
tt_topics <- tidy(trump_tweet_lda, matrix = "beta")
tt_topics
tt_top_term <-
tt_topics %>%
group_by(topic) %>%
top_n(10, beta) %>%
ungroup() %>%
arrange(topic, desc(beta))
tt_top_term
tt_top_term %>%
# mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
tt_top_term %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(x = term, y = beta, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
coord_flip() +
facet_wrap(~ topic, scales = "free")
#Adjust your working directory here
knitr::opts_knit$set(root.dir = '/Volumes/Workspace/OneDrive - Universite de Montreal/Canadian_Newspapers/Canadian newspaper')
rm(list = ls())
# Voici la liste des packages utilisés pour cet article
library('data.table')
library('tidytext')
library('tidyverse')
library('lubridate')
library('furrr')
library('future')
library('prettydoc')
install.packages("prettydoc")
df1 <- read_csv("newspapers.csv")
df1 <- read_csv("newspapers.csv")
#Adjust your working directory here
knitr::opts_knit$set(root.dir = '/Volumes/Workspace/OneDrive - Universite de Montreal/Canadian_Newspapers/Canadian newspaper')
df1 <- read_csv("newspapers.csv")
