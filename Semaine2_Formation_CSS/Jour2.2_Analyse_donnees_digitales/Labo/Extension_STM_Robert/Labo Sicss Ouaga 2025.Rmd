---
title: "Analyse quantitative de texte"
author:
- Robert Djogbenou:
    email: yao.robert.djogbenou@umontreal.ca
    institute: Demo
    correspondence: yes
  institute: Demo
institute:
- Demo: D√©partement de D√©mographie, Universit√© de Montr√©al
output:
  word_document:
    reference_docx: word-styles-reference.docx
    pandoc_args:
    - --csl=Extras/apa7-fr-couture.csl
    - --citation-abbreviations=Extras/abbreviations.json
    - --filter=pandoc-crossref
    - --lua-filter=Extras/scholarly-metadata.lua
    - --lua-filter=Extras/author-info-blocks.lua
  fig_caption: yes
  papersize: a4
  html_document:
    df_print: paged
  pdf_document: default
chunk_output_type: inline
editor_options: null
mainfont: Arial
fontsize: 12pt
geometry: left=2.5cm, right=2.5cm, top=2.5cm, bottom=2.5cm
linestretch: 1.15
secnumdepth: 1
bibliography:
- Ma_bibliotheque.bib
- Women_representation_in_media.bib
link-citations: yes
linkcolor: blue
csl: apa.csl
keep-latex: yes
---

## Plan du labo

1. Topic modeling (STM)
2. Sentiment analysis
3. Combinaison de STM, sentiment analysis avec modeles longitudinaux

## Introduction

Avec le nombre croissant de donn√©es textuelles et archives num√©riques (donn√©es non structur√©es), il devient de plus en plus difficile 
d‚Äôextraire des informations pertinentes et n√©cessaires dans ces donn√©es. Ces donn√©es donnent un aper√ßu sans pr√©c√©dent des questions 
fondamentales en sciences sociales. Pendant ce temps, plusieurs techniques statistiques ont √©t√© d√©velopp√©es pour explorer et r√©cup√©rer 
les informations pertinentes et n√©cessaires dans les donn√©es textuelles. Dans ce labo, nous donnons un bref aper√ßu des techniques de la 
mod√©lisation des th√®mes et de l‚Äôanalyse de sentiment.

La mod√©lisation des th√®mes est une technique d‚Äôapprentissage automatique non supervis√©e utilis√©e pour d√©couvrir des th√®mes latents cach√©s 
contenus dans un texte et des relations qui n‚Äôont pas √©t√© analys√©s et cod√©s manuellement au pr√©alable. Elle diff√®re de l‚Äôanalyse qualitative 
du contenu ou de l‚Äôanalyse du discours, en ce sens qu‚Äôelle tente de r√©duire la complexit√© et de rendre les donn√©es aptes aux inf√©rences 
statistiques. L‚Äôid√©e derri√®re la mod√©lisation des th√®mes est que, des algorithmes identifient des mots similaires dans des documents d‚Äôun 
corpus donn√© et les regroupent en th√®mes, permettant ainsi d‚Äôidentifier le contenu th√©matique de ce corpus.

Ce processus est similaire √† l‚Äôanalyse factorielle. Il existe g√©n√©ralement trois processus pour les mod√®les th√©matiques. 
Le premier processus est similaire aux facteurs dans l‚Äôanalyse factorielle et consiste √† l‚Äôextraction de l‚Äôensemble des th√®mes apparus dans 
les documents. Le deuxi√®me processus, similaire aux charges factorielles, consiste √† d√©terminer la probabilit√© d‚Äôapparition des mots 
dans chaque th√®me. Le troisi√®me processus, √† peu pr√®s similaire aux scores factoriels, est la proportion de chaque document qui 
peut √™tre attribu√©e √† chaque th√®me. 

Un mod√®le th√©matique largement utilis√© est le Latent Dirichlet Allocation (LDA), propos√© par Blei et al. (2003). 
Nous utilisons une extension r√©cente du LDA appel√©e Structural Topic Modeling (STM) (Roberts et al., 2014, 2019). 
La particularit√© du STM est sa capacit√© √† int√©grer des m√©tadonn√©es des documents (par exemple, l‚Äôann√©e, la source, 
ou toute autre variable cat√©gorielle ou num√©rique) afin d‚Äôam√©liorer l‚Äôestimation de la pr√©valence et du contenu 
des sujets. Il a √©t√© d√©montr√© que l‚Äôajout de covariables am√©liore 
significativement la qualit√© des sujets, et que l‚Äôinclusion de la date 
est particuli√®rement utile pour analyser les √©volutions temporelles et les changements de discours.

En r√©sum√©, STM diff√®re principalement de LDA sur trois points : premi√®rement, les th√®mes peuvent √™tre corr√©l√©s ; deuxi√®mement, 
chaque document a sa propre distribution de th√®mes, d√©finie par les covariables plut√¥t que de partager une moyenne globale ; 
troisi√®mement, l‚Äôutilisation des mots dans un th√®me peut varier selon les variables ind√©pendantes.


# D√©finir le r√©pertoire de travail (working directory)

```{r}
# Adjust your working directory here
knitr::opts_knit$set(root.dir = '')
```

# Nettoyage de l'environnement de travail

```{r}
rm(list = ls())
```

############ Voici la liste des packages utilis√©s pour cet article ############

```{r library, include=FALSE}
 
# Liste des packages avec explication du r√¥le

packages <- c(
  "tidyverse",     # Ensemble de packages pour manipulation et visualisation (inclut dplyr, ggplot2, etc.)
  "data.table",    # Lecture et manipulation rapide de donn√©es
  "readxl",        # Lecture de fichiers Excel (.xlsx)
  "writexl",       # √âcriture de fichiers Excel (.xlsx)
  "quanteda",      # Analyse quantitative de texte (NLP)
  "quanteda.textstats",  # Fonctions avanc√©es de statistiques textuelles (ex : d√©tection de collocations, calculs statistiques sur les tokens)
  "spacyr",        # Interface R pour spaCy (NLP en Python) : tokenisation, entit√©s nomm√©es, d√©pendances, etc.
  "tidytext",      # Traitement de texte au format tidy (tokenisation, tf-idf, etc.)
  "stm",           # Mod√©lisation th√©matique structur√©e (Structural Topic Models)
  "lubridate",     # Manipulation de dates
  "parsedate",     # Parsing de formats de dates vari√©s
  "igraph",        # Analyse et visualisation de r√©seaux
  "wordcloud2",    # Nuages de mots interactifs en HTML
  "wordcloud",     # Nuages de mots simples (statique)
  "summarytools",  # Statistiques descriptives jolies et compl√®tes
  "textstem",      # Lemmatisation et traitement de texte
  "future",        # Traitement parall√®le (utilis√© avec `stm`)
  "RColorBrewer",  # Palettes de couleurs
  "scales",        # Mise √† l‚Äô√©chelle pour les visualisations (ex: format pour les axes)
  "ggthemes",      # Th√®mes suppl√©mentaires pour ggplot2
  "furrr",         # Version parall√®le de purrr (map() + future)
  "prettydoc",     # Th√®mes esth√©tiques pour documents RMarkdown
  "snakecase",     # Conversion de noms (e.g., camelCase ‚Üí snake_case)
  "hrbrthemes",    # Th√®mes professionnels pour ggplot2
  "extrafont",     # Importation et utilisation de polices suppl√©mentaires
  "car",           # Outils pour analyse de r√©gression (tests, ANOVA, etc.)
  "ff",            # Manipulation efficace de grands jeux de donn√©es (hors RAM)
  "htmlwidgets",   # Export HTML interactif (utile pour `wordcloud2`)
  "webshot",       # Capture de widgets HTML en PNG ou PDF
  "ggrepel",       # √âtiquettes intelligentes dans ggplot2 (√©vite chevauchement)
  "cowplot",       # Combinaison de plusieurs graphes ggplot2
  "stargazer",     # Tableaux de mod√®les (r√©gressions) jolis en LaTeX, HTML, texte
  "labelled",      # Gestion des √©tiquettes de variables (utile pour donn√©es d‚Äôenqu√™tes)
  "rstatix",       # Statistiques descriptives et inf√©rentielles simples
  "ggpubr",        # Statistiques et visualisations publication-ready
  "GGally",        # Extensions ggplot2 (ex: ggpairs pour matrices de corr√©lation)
  "Epi",           # Statistiques √©pid√©miologiques (IC, RR, OR, etc.)
  "lme4",          # Mod√®les mixtes (effets fixes et al√©atoires)
  "lmerTest",      # P-values pour lme4
  "emmeans",       # Moyennes marginales estim√©es (comparaisons post-hoc)
  "multcomp",      # Intervalles de confiance pour combinaisons lin√©aires
  "geepack",       # Mod√®les √† √©quations d‚Äôestimation g√©n√©ralis√©es (GEE)
  "ggeffects"      # Effets marginaux et pr√©dictions ajust√©es (ggplot friendly)
)

# Charger tous les packages avec une boucle

invisible(lapply(packages, library, character.only = TRUE))

```


# Chargement des donn√©es

```{r message=FALSE, warning=FALSE}

df <- fread("df_ontario.csv")
```


################# Analyse de texte ##################

https://content-analysis-with-r.com/4-dictionaries.html


################# PARTIE I: ANALYSE DE TEXTE : STM ##########################################


```{r}
# Installer spaCy et ses d√©pendances Python (√† faire une seule fois si n√©cessaire)
# spacy_install()  # D√©commentez cette ligne si spaCy n'est pas encore install√©

# Initialiser spaCy et charger le mod√®le linguistique l√©ger en anglais

spacy_initialize(model = "en_core_web_sm")  # Ce mod√®le est rapide, mais moins pr√©cis que les mod√®les transformers

```


```{r}
# Analyse linguistique du texte avec spaCy via spacyr

spacy <- spacy_parse(df$text, 
                             lemma = TRUE,   # Retourne les lemmes (formes de base des mots)
                             pos = TRUE      # Inclut les parties du discours (noms, verbes, etc.)
                             # by default, entities = FALSE
)

```


```{r}
saveRDS(spacy, 'spacy.rds')
```


```{r}
#spacy <- read_rds("spacy.rds")

```


# Adds stopwords

```{r message=FALSE, warning=FALSE, results = 'hide'}

library(readxl)
library(dplyr)

# Lire le fichier Excel
mystopwords <- read_excel("stopwords_final.xls")

# Supprimer les doublons et les NA
mystopwords <- mystopwords %>%
  distinct(words, .keep_all = TRUE) %>%
  drop_na()

# Nettoyer les espaces en trop
mystopwords <- trimws(as.character(mystopwords$words))

```


```{r}
# Transforme les r√©sultats de spacy_parse en tokens avec lemmatisation
newspapers_tokens <- as.tokens(spacy, use_lemma = TRUE) %>%

  # Nettoyage de base : suppression de ponctuations, chiffres, symboles, s√©parateurs, URLs
  quanteda::tokens(remove_punct = TRUE, 
                   remove_numbers = TRUE, 
                   remove_symbols = TRUE,
                   remove_separators = TRUE,
                   remove_url = TRUE,
                   split_hyphens = TRUE,
                   include_docvars = TRUE) %>%

  # Mise en minuscules
  tokens_tolower() %>%

  # Suppression des stopwords personnalis√©s
  quanteda::tokens_remove(pattern = phrase(mystopwords), valuetype = 'fixed') %>%

  # Suppression des stopwords standards en anglais (avec remplissage si besoin)
  quanteda::tokens_remove(stopwords("en"), padding = TRUE)

```

## Nettoyer davantage les tokens apr√®s le pr√©traitement initial avec quanteda 

```{r}
# üîç Nettoyage final des tokens : suppression des ponctuations et mots tr√®s courts
newspapers_tokens <- tokens_select(
  newspapers_tokens,
  pattern = c("[[:punct:]]", "^.{1,2}$"),  # mots de 1 ou 2 lettres, ponctuations
  selection = "remove",
  valuetype = "regex",   # on utilise des expressions r√©guli√®res
  padding = TRUE,        # conserve la structure si n√©cessaire
  verbose = TRUE         # affiche les logs
)

```

```{r}
# √âtape 1 : Identifier les collocations (groupes de mots fr√©quents, ex : "prime minister")
coll <- textstat_collocations(newspapers_tokens, min_count = 50)

# √âtape 2 : Fusionner les collocations dans les tokens (optionnel)
newspapers_tokens <- tokens_compound(newspapers_tokens, coll, join = FALSE)

# √âtape 3 : Remplacer des expressions sp√©cifiques par un mot unifi√©
# Exemple : transformer "per cent" ‚Üí "percent"
bigrams <- c("per cent")
unigrams <- c("percent")

newspapers_tokens <- tokens_replace(
  newspapers_tokens,
  pattern = bigrams,
  replacement = unigrams
)
```


# Cr√©ation de **document-feature matrix (DFM)** 


```{r}
# Cr√©ation d'une matrice document-terme (DFM) √† partir des tokens nettoy√©s
# Filtrage des termes dans la DFM : 
# - suppression des termes tr√®s rares (apparaissant dans moins de 7,5% des documents)
# - suppression des termes tr√®s fr√©quents (pr√©sents dans plus de 80% des documents)
# Cela permet de garder un vocabulaire pertinent et d'√©viter le bruit caus√© par termes trop sp√©cifiques ou trop g√©n√©raux.
# Supprime tous les tokens dont la longueur est inf√©rieure √† 3 caract√®res


dfm_newspapers <- newspapers_tokens %>% 
  dfm(tolower = TRUE) %>%  # Transformer les tokens en document-feature matrix
  dfm_remove(min_nchar = 3) %>%  # Supprimer les tokens < 3 caract√®res
  dfm_trim(min_termfreq = 0.075, termfreq_type = "quantile",
           max_docfreq = 0.80, docfreq_type = "prop") %>%  # Filtrer termes trop rares ou trop fr√©quents
  quanteda::dfm_subset(quanteda::ntoken(.) > 0)  # Supprimer documents vides (sans tokens)

```


```{r}
cloud_dfm <- colSums(dfm_newspapers)

sum(ntoken(dfm_newspapers))

```

```{r}
# Affiche les 10 premi√®res observations (documents) et les 10 premi√®res caract√©ristiques (termes) 
# de la matrice document-terme (DFM) tri√©e par fr√©quence d√©croissante √† la fois pour les documents et les termes.
head(dfm_sort(dfm_newspapers, 
              decreasing = TRUE, margin = "both"), n = 10)

```

# Wordcloud visualization


```{r fig.height=5, fig.width=7}

# Visualisation par nuage de mots (wordcloud)
# Le wordcloud montre les termes les plus fr√©quents dans tous les articles,
# o√π la taille des mots refl√®te leur fr√©quence d'apparition.

# webshot::install_phantomjs(force = TRUE) # √† d√©commenter si n√©cessaire pour sauvegarder en image

set.seed(1234) # pour garantir la reproductibilit√© du nuage

# Cr√©ation du nuage de mots avec wordcloud2
# - data.frame : donn√©es avec termes et leurs fr√©quences
# - size : taille globale du nuage
# - ellipticity : forme elliptique du nuage
# - shuffle : ordre des mots non m√©lang√© pour coh√©rence visuelle
# - shape : forme g√©n√©rale du nuage (ici cercle)
# - color : couleur al√©atoire sombre
# + WCtheme(1) : applique un th√®me graphique esth√©tique

my_graph <- wordcloud2(data.frame(names(cloud_dfm), cloud_dfm),
           size = 1.6, ellipticity = 1, shuffle = FALSE, shape = 'circle',  color='random-dark') + WCtheme(1)

# Affichage du nuage de mots
my_graph


```

# Mod√®les de sujets structurels (Structural Topic Modeling, STM) - apprentissage non supervis√©

STM est une extension des mod√®les LDA classiques, qui int√®gre des m√©tadonn√©es (informations sur chaque document) 
dans la mod√©lisation des sujets.

Cette approche permet notamment :
 - d‚Äôinclure des covariables dans les priorit√©s du mod√®le
 - d‚Äôutiliser une m√©thode d'initialisation alternative ("Spectral")


```{r}
# https://quanteda.io/articles/pkgdown/quickstart.html


# Pour appliquer STM avec les donn√©es pr√©par√©es dans quanteda,
# il faut d'abord convertir la matrice document-terme (DFM) 
# en format compatible STM.

# La fonction quanteda::convert permet de convertir la DFM en objet STM, 
# en joignant les variables documentaires (docvars) extraites de df.

out <- quanteda::convert(dfm_newspapers, to = "stm",
                         docvars = df
                         )

```


```{r}
saveRDS(out, file = "out.rds")

```


```{r}
#out <- read_rds("out.rds") 
```


# S√âLECTION DU MOD√àLE STM (Structural Topic Model)

Objectif : d√©terminer le nombre optimal de topics (K) pour le mod√®le STM.
On teste plusieurs valeurs de K, allant de 5 √† 50, 
La m√©thode d'initialisation "Spectral" est utilis√©e car elle donne des r√©sultats optimaux.
Le mod√®le prend en compte une covariable de pr√©valence : ici, l'ann√©e ("year").
Le nombre maximal d'it√©rations EM (expectation-maximization) est fix√© √† 50 pour chaque mod√®le.

# La parall√©lisation avec future.apply (ici 5 workers) acc√©l√®re l'entra√Ænement sur plusieurs c≈ìurs CPU.

## Attention, le code prend plus de temps pour converger

```{r}

library(future.apply)  # Permet d'ex√©cuter des boucles et fonctions en parall√®le pour acc√©l√©rer le calcul, 
# notamment utile pour entra√Æner plusieurs mod√®les STM simultan√©ment.

# Activer la parall√©lisation avec 5 workers
plan(multisession, workers = 5)

# Afficher la configuration de planification actuelle (facultatif)
plan()

# Augmenter la limite m√©moire pour la parall√©lisation (ici 8 Go)
options(future.globals.maxSize = 8000 * 1024^3)

# Liste des valeurs de K √† tester
Ks <- c(5:25)

# Ex√©cuter les mod√®les STM pour chaque K
many_models <- tibble(K = Ks) %>% 
  mutate(model = map(K, ~ stm(
    documents = out$documents,     # documents pr√©par√©s pour STM
    vocab = out$vocab,             # vocabulaire STM
    data = out$meta,               # m√©tadonn√©es, incluant la variable "year"
    K = .,                        # nombre de topics pour ce mod√®le
    prevalence = ~ year,           # covariable de pr√©valence : l'ann√©e
    init.type = "Spectral",        # m√©thode d'initialisation
    seed = TRUE,                   # seed pour reproductibilit√©
    max.em.its = 50,               # nombre max d'it√©rations EM
    verbose = TRUE                 # afficher les logs d'entra√Ænement
  )))

```


```{r}
#SAVING RESULTS

saveRDS(many_models, file = "many_model.rds")

```


```{r}
#many_models <- read_rds("many_model.rds")
```


```{r}
heldout <- make.heldout(dfm_newspapers)
# Cr√©e un jeu de donn√©es "held-out" √† partir de la matrice document-terme (dfm),
# utilis√© pour √©valuer la performance pr√©dictive du mod√®le STM sur des donn√©es non vues pendant l'entra√Ænement.

```


```{r}
saveRDS(heldout , file = "heldout.rds")
```

## √âvaluation des modeles

Ce code permet d'√©valuer plusieurs mod√®les STM avec diff√©rents nombres de topics (K) en utilisant 
plusieurs m√©triques diagnostics (exclusivit√©, coh√©rence s√©mantique, held-out likelihood, r√©sidus) 
et facilite ainsi la s√©lection du meilleur nombre de topics.

```{r}

k_result <- many_models %>%
  mutate(
    # Calcul de l'exclusivit√© des mots pour chaque mod√®le
    exclusivity = map(model, exclusivity),
    
    # Calcul de la coh√©rence s√©mantique des topics pour chaque mod√®le, avec la DFM utilis√©e
    semantic_coherence = map(model, semanticCoherence, dfm_newspapers),
    
    # √âvaluation de la performance pr√©dictive sur les donn√©es "held-out"
    eval_heldout = map(model, eval.heldout, heldout$missing),
    
    # Analyse des r√©sidus pour chaque mod√®le (diagnostic de la qualit√© du fit)
    residual = map(model, checkResiduals, dfm_newspapers),
    
    # Extraction de la borne sup√©rieure de la fonction objectif pour chaque mod√®le
    bound =  map_dbl(model, function(x) max(x$convergence$bound)),
    
    # Calcul du logarithme de la factorielle du nombre de topics (dimension) pour chaque mod√®le
    lfact = map_dbl(model, function(x) lfactorial(x$settings$dim$K)),
    
    # Calcul d'une borne ajust√©e qui combine bound et lfact (potentiellement pour comparaison)
    lbound = bound + lfact,
    
    # Nombre d‚Äôit√©rations jusqu‚Äô√† convergence pour chaque mod√®le
    iterations = map_dbl(model, function(x) length(x$convergence$bound))
  )

# Affiche les r√©sultats d'√©valuation pour tous les mod√®les test√©s
k_result

```


```{r}
saveRDS(k_result, 'k_result.rds')

```


```{r}
#k_result <- read_rds('k_result.rds')

#k_result
```

# Visualiser ces quatre m√©triques cl√©s par nombre de topics (K)


Semantic coherence : mesure √† quel point les mots d‚Äôun m√™me topic apparaissent dans les m√™mes documents.

Exclusivity : mesure √† quel point les mots d‚Äôun topic sont exclusifs √† ce topic.

L‚Äôobjectif est d‚Äôidentifier les mod√®les situ√©s dans le coin sup√©rieur droit : 
ils combinent √† la fois une bonne coh√©rence s√©mantique et une bonne exclusivit√© ‚Üí mod√®les de qualit√©.



```{r fig.height=5, fig.width=7}

results_plot <- k_result %>%
  transmute(
    K,  # Nombre de topics
    
    # Moyenne de l'exclusivit√© des mots (plus √©lev√© = topics plus distincts)
    Exclusivity = map_dbl(exclusivity, mean),
    
    # Moyenne de la coh√©rence s√©mantique (plus √©lev√© = topics plus coh√©rents s√©mantiquement)
    `Semantic coherence` = map_dbl(semantic_coherence, mean),
    
    # Dispersion des r√©sidus (diagnostic sur la qualit√© du mod√®le, plus bas est mieux)
    `Dispersion of residuals` = map_dbl(residual, "dispersion"),
    
    # Log-vraisemblance sur donn√©es "held-out" (plus haut = meilleure capacit√© pr√©dictive)
    `Held-out likelihood` = map_dbl(eval_heldout, "expected.heldout")
  )

```


```{r fig.height=8, fig.width=17}

results_plot_graph <- results_plot %>%
  # Transformation du tableau au format long pour ggplot
  gather(Metric, Value, -K) %>%
  
  # Cr√©ation du graphique avec ggplot2
  ggplot(aes(K, Value, color = Metric)) +
  
  # Ligne par m√©trique
  geom_line(linewidth = 1, alpha = 0.7, show.legend = FALSE) + 
  
  # Points pour chaque valeur de K
  geom_point(size = 1) +
  
  # Un graphique s√©par√© (facette) par m√©trique, avec √©chelle Y propre
  facet_wrap(~Metric, scales = "free_y") +
  
  # Titres des axes
  labs(x = "Topic Number", y = NULL) +
  
  # Th√®me blanc √©pur√©
  theme_bw() +
  
  # Suppression des lignes de grille
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank()
  ) +
  
  # Mise en forme de la l√©gende et des textes
  theme(
    legend.direction = "horizontal",
    legend.position = "none",
    legend.title = element_blank(),
    axis.text.x = element_text(size = 14, color = "black"),
    axis.title.x = element_text(size = 14, color = "black"),
    strip.text.x = element_text(size = 14, color = "black"),
    axis.text.y = element_text(size = 14, color = "black"),
    axis.title.y = element_text(size = 14, color = "black"),
    strip.text.y = element_text(size = 14, color = "black")
  )

# Affichage du graphique

results_plot_graph

```

# Visualiser les performances de chaque mod√®le STM en fonction de K √† partir de deux crit√®res qualitatifs :


```{r fig.height=8, fig.width=15}

# Pr√©paration des donn√©es
results_dots_data <- results_plot %>%
  dplyr::select(K, `Semantic coherence`, Exclusivity) %>%
  unnest(cols = c()) %>%  # N√©cessaire uniquement si ce sont des list-cols
  mutate(topic = paste0("K", K))  # Cr√©ation de labels du type K5, K10, etc.

# Cr√©ation du graphique
results_dots <- ggplot(results_dots_data, aes(x = `Semantic coherence`, y = Exclusivity, label = topic)) +
  geom_point(alpha = 0.5, color = "red") +
  geom_text_repel(min.segment.length = 0, seed = 42, box.padding = 0.30, size = 5) +
  labs(x = "Semantic coherence", y = "Exclusivity") +
  theme_bw() + 
  theme(
    panel.grid.major.x = element_blank(),
    panel.grid.major.y = element_blank(),
    panel.grid.minor.y = element_blank(),
    panel.grid.minor.x = element_blank(),
    axis.text.x = element_text(size = 14, color = "black"),
    axis.title.x = element_text(size = 14, color = "black"),
    axis.text.y = element_text(size = 14, color = "black"),
    axis.title.y = element_text(size = 14, color = "black"),
    strip.text.x = element_text(size = 14, color = "black"),
    strip.text.y = element_text(size = 14, color = "black")
  )

results_dots

```

## MODELING

# Specification for K topics using stm function 

# According to the previous graphs, the number of topics is 20, maximal number of iteration is 70, used method is spectral as according to the guidelines it provides "optimal results". Gamma prior is L1 as with such a small number of iterations the model would have otherwise not converged. Can be removed if the n. of iterations is increased to allow model to converge. We identify the covariate we are interested in under prevalence.


```{r}
topic_model <- k_result %>% 
  filter(K == 14) %>% 
  pull(model) %>% 
  .[[1]]

topic_model
```


```{r}
#SAVING RESULTS

saveRDS(topic_model, file = "topic_model.rds")

```


```{r}
topic_model <- read_rds('topic_model.rds') # loads topic_model
```


# Representation des topics (24 topics are good)


```{r fig.height=8.5, fig.width=13.5, warning=FALSE}

topic <- plot(topic_model, 
              n=5, 
              type = "summary", 
              xlim = c(0,.25), 
              labeltype = c("prob"),
              
              main="", 
              text.cex=1.1, 
              xlab="", 
              cex.axis = 1.5, 
              col = alpha(1), 
              pch=26)
mtext("Average Topic Proportions", 
      side=1, 
      line=3, 
      cex=1.5) #adds custom Y-axis label

topic
  
```


## Words clouds

```{r fig.height=10, fig.width=12, warning= FALSE, message=FALSE}

set.seed(1) # for reproducibility 

plot.new()

A= c(1:14)

par(mfrow=c(5,4), mar=c(0, 0, 2, 0.5))
for (i in A)
{
stm::cloud(topic_model, 
           topic = i, 
           scale = c(8,.7),
           rot.per=0,
           fixed.asp = FALSE, 
           min.freq=10,
           random.order=FALSE,
           random.color=TRUE,  
           colors=brewer.pal(max(8,
                                 ncol(topic_model)),
                             "Dark2"))
title(main=topic[i], 
      font.main=4, 
      col.main="black", 
      cex.main=1.7) 
}

```

# Tidy approche

```{r}

td_beta <- tidy(topic_model)# Extraire la distribution des mots (beta) pour chaque topic dans un format tidy (long)

#td_beta
```

# # Extraire la distribution des topics (gamma) pour chaque document
# Chaque ligne donne la probabilit√© qu‚Äôun document soit associ√© √† un topic donn√©

Note : matrix = "gamma" permet d‚Äôobtenir les proportions th√©matiques par document ‚Äî c‚Äôest la matrice fondamentale 
pour visualiser la r√©partition des themes dans le corpus.

```{r include=FALSE}

td_gamma <- tidy(topic_model, matrix = "gamma",
                 document_names = rownames(out))

td_gamma

```

## Extraire les 5 termes les plus repr√©sentatifs (avec les plus fortes probabilit√©s Œ≤) pour chaque topic
# Puis, calculer la moyenne des probabilit√©s gamma pour chaque topic (importance moyenne dans le corpus)
# Enfin, fusionner les deux pour afficher les topics les plus pr√©sents accompagn√©s de leurs termes caract√©ristiques


```{r fig.height=1.5, fig.width=3, warning=FALSE, message=FALSE}

library(ggthemes)


top_terms <- td_beta %>%
  arrange(beta) %>%
  group_by(topic) %>%
  top_n(5, beta) %>%
  arrange(-beta) %>%
  dplyr::select(topic, term) %>%
  dplyr::summarise(terms = list(term)) %>%
  mutate(terms = map(terms, paste, collapse = ", ")) %>% 
  unnest(cols = c(terms))

gamma_terms <- td_gamma %>%
  group_by(topic) %>%
  dplyr::summarise(gamma = mean(gamma)) %>%
  arrange(desc(gamma)) %>%
  left_join(top_terms, by = "topic") %>%
  mutate(topic = paste0("Topic ", topic),
         topics = reorder(topic, gamma))

```

# Plot topic graph


```{r fig.height=8, fig.width=16}
library(scales)  # pour percent_format()

library(forcats)

topic_plot <- gamma_terms %>%
  top_n(20, gamma) %>%
  mutate(topic = fct_reorder(topic, gamma, .desc = FALSE)) %>%
  ggplot(aes(topic, gamma, label = terms, fill = topic)) +
  geom_col(show.legend = FALSE) +
  geom_text(hjust = 0, nudge_y = 0.0025, size = 5, color = "black",
            family = "Arial") +
  coord_flip() +
  scale_y_continuous(expand = c(0, 0),
                     limits = c(0, 0.18),
                     labels = scales::percent_format()) +
  ggthemes::theme_tufte(base_family = "Arial", ticks = FALSE, base_size = 13) +
  theme(
    plot.title = element_text(size = 17, family = "Arial"),
    axis.text.x = element_text(size = 13, color = "black"),  # taille des labels x
    axis.text.y = element_text(size = 13, color = "black")   # taille des labels y
  ) +
  labs(x = NULL, y = "Expected Topic Proportions")

topic_plot

```

```{r}
# POST-ESTIMATION DIAGNOTICS

# Displaying words associated with topics or documents highly associated with particular topics

# Find prototype documents using function findThoughts()

findThoughts(topic_model, texts = out$meta$R√©sum√©, topics = c(3),  n=1)$docs[[1]] 

```

```{r echo=FALSE}
out$meta <- as.data.frame(out$meta)

prep_interaction_year <- estimateEffect(
  1:14 ~ s(year), 
  topic_model, 
  meta = out$meta, 
  uncertainty = "Global", 
  prior = 1e-5
)


```


# Evolution des topics dans le temps 


```{r}

library(stminsights)

effects_year <- get_effects(estimates = prep_interaction_year,
                      variable = 'year',
                      type = 'continuous')

```


```{r fig.height=12, fig.width=19}

effects_year_graph <- effects_year %>%
  mutate(topic = paste0("Topic ", topic))%>%

 ggplot(aes(x = value, y = proportion, color = topic,
 group = topic, fill = topic)) +
geom_line(stat = "identity", position = "identity", size=.8, linewidth=0.8) +
#geom_ribbon(aes(ymin = lower, ymax = upper), alpha = 0.2)  +
  labs(x = "",
       y = "Topic proportion") +
  theme_bw() + 
  theme(axis.text.x = element_text(color = "black", size = 16),
        panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.major.y = element_blank(),
        panel.grid.minor.y = element_blank())+
  theme(legend.position = "none") +
  theme(axis.title.x  = element_text(color = "black", size = 18))+
  theme(axis.text.x = element_text(color = "black", size = 18))+
  theme(axis.text.y = element_text(color = "black", size = 18))+
  theme(axis.title.y  = element_text(color = "black", size = 18))+
  scale_color_discrete(name = "")+
  theme(legend.text = element_text(size = 18))+
  theme(plot.title = element_text(color = "black", size = 18, lineheight = 0.5),
        legend.title.align = NULL) +
  scale_x_continuous(breaks=c(1988, 1993, 1998, 2003, 2008, 2013, 2018, 2022)) +
  facet_wrap(~ topic) +
  theme(strip.text = element_text(size = 18)) +
  scale_fill_discrete(name="") +
  theme(axis.text.x=element_text(angle = 90, vjust = 0.5))

effects_year_graph

```



```{r include=FALSE}

library(compositions)

theta20 <- topic_model$theta  #theta14 is the document-topic matrix

cd<-acomp(theta20) #the transformation

dd <- as.dist(variation(cd))

hc = hclust(dd, method="ward.D2")

```


```{r fig.height=8, fig.width=12, warning=FALSE}

library(dendextend)

# Convertir hc en dendrogramme
d <- as.dendrogram(hc)

# Modifier les labels en ajoutant "Topic " devant chaque num√©ro
labels(d) <- paste0("Topic ", labels(d))

# Colorer les branches (exemple k = 8)
d_colored <- color_branches(d, k = 4)

# Afficher avec les labels modifi√©s
par(mar = c(4, 4, 2, 2))  # marges

plot(d_colored,
     ylab = "Distance",
     cex = 1,         # taille des labels des feuilles (ex: "Topic 1")
     cex.lab = 1,     # taille du label de l'axe y ("Distance")
     cex.axis = 1,      # taille des chiffres sur les axes
     cex.main = 2     # taille du titre principal (si tu en mets un)
)

```


# Calculating topic correlations (topicCorr)



```{r}
library(network)
library(visNetwork)
library(igraph)
library(statnet)
library(intergraph)
library(devtools)
library("devtools")
#install_github("DougLuke/UserNetR")
#library(UserNetR)
```



```{r}
# Topic correlations using function topicCorr()
mod.out.coor <- topicCorr(topic_model, method = "simple", cutoff = 0)
# create objects representing topic scores & correlation matrix
theta <- topic_model$theta
cor_theta <- cor(theta)

```


# Network analysis : Graphical display of topic correlations.

# Topic Correlation Graph (Tie strength > 0.01). Node size reflects corpus-level topic proportion. Ties indicate greater likelihood that topics are discussed within common documents. Coloring emphasizes topic clusters.


```{r}
topic.numbers=c(1:14)

topic.labels<- paste0("Topic ", 1:14)
```



```{r fig.height=17, fig.width=18, warning=FALSE}
m <- mod.out.coor$poscor
m<-m[c(1:14),c(1:14)]   # sous-matrice 14x14

net<-network(m,ignore.eval=FALSE,names.eval="like", directed=FALSE)
network.vertex.names(net)<-topic.labels
g<-asIgraph(net)
#plot(g)

modularity(cluster_louvain(g))
modularity(cluster_edge_betweenness(g))
modularity(cluster_leading_eigen(g))

set.seed(123)
cl<-cluster_louvain(g)
#plot(cl,g)
modularity(cl)
V(g)$community<-cl$membership
nodes<-data.frame(topic=c(1:14),id=V(g)$vertex.names, group=V(g)$community)

topic.nl<-data.frame(label=topic.labels, number=topic.numbers)
topic.nl$seq=seq(1:nrow(topic.nl))

edge<-get.data.frame(g,what="edges")  


colnames(topic.nl)[3]<-"from"
edges<-edge%>%left_join(topic.nl,by="from")
edges$from=edges$number
colnames(topic.nl)[3]<-"to"
edges<-edges%>%left_join(topic.nl,by="to")
edges$to=edges$number.y
edges<-edges%>% dplyr::select(label.x,label.y)
colnames(edges)[1:2]<-c("from","to")

graph<-graph.data.frame(edges,directed=F)
degree_value<-igraph::degree(graph) ##node size= the number of degrees
nodes$value<-degree_value[match(nodes$id,names(degree_value))]
```


```{r fig.height=17, fig.width=30, warning=FALSE}

network <- visNetwork(nodes,
                   edges, 
                   width = "100%", 
                   height = 900)%>%
  visOptions(highlightNearest = T, nodesIdSelection = F) %>%
  visInteraction(navigationButtons = F, 
                 dragNodes = T,
                 dragView = T, 
                 zoomView = TRUE)%>%
  visNodes() #%>%
  #visEdges(arrows = 'to')%>%
  #visLegend() 

  #visSave(file = "network.html")

network
```


## Extraction des proportions des th√®mes
    
## Nous utilisons make.dt() pour r√©cup√©rer le document-topic-matrix()  (les probabilit√©s des 20 th√®mes dans chaque document)

```{r}
theta <- make.dt(topic_model)

```


# Merger les topics avec les donn√©es originales


```{r}

# Sauvegarde temporaire de l'objet 'out' dans 'out_reserve' pour conserver les donn√©es originales
out_reserve <- out

# Ajout d'une colonne d'identification unique 'ID' dans les m√©tadonn√©es pour chaque article
out$meta$ID <- seq.int(nrow(out$meta))

# Extraction de la matrice des proportions de topics (theta) issue du mod√®le, convertie en dataframe
theta <- data.frame(topic_model$theta)

# Attribution d'un identifiant unique 'ID' aux lignes de la matrice theta, correspondant aux documents
theta$ID <- seq.int(nrow(theta))

# Fusion des m√©tadonn√©es et de la matrice theta via la colonne 'ID' pour obtenir un dataframe complet
full_data_large <- merge(out$meta, theta)

# Suppression des colonnes dupliqu√©es cr√©√©es lors de la fusion (suffixes .x et .y)
full_data_large <- full_data_large %>%
  dplyr::select(-ends_with(c(".x", ".y")))

```


#################### PARTIE 2 : ANALYSE DE SENTIMENT (TON) ###############################################

## Sentiment global par document

```{r}
## Appliquer un dictionnaire de sentiment √† une matrice document-terme
dfm_lsd <-
  dfm_lookup(dfm_newspapers,                      # Applique un dictionnaire √† la DFM dfm_newspapers
             dictionary = data_dictionary_LSD2015[1:2], # Utilise les deux premi√®res cat√©gories du dictionnaire LSD2015 (positive et negative)
             exclusive = TRUE,                   # Ignore les mots qui ne figurent pas dans le dictionnaire
             case_insensitive = TRUE,            # Ignore la casse (majuscules/minuscules)
             verbose = FALSE) %>%
  # convertit ensuite la DFM en data.frame (format tabulaire)
  convert(to = "data.frame") %>%
  mutate(
    total_words = ntoken(dfm_newspapers),       # Calcule le nombre total de mots dans chaque document
    pos_perc = 100 * positive / total_words,    # Calcule le pourcentage de mots positifs
    neg_perc = 100 * negative / total_words,    # Calcule le pourcentage de mots n√©gatifs
    net_perc = pos_perc - neg_perc              # Calcule le score net de sentiment (positif - n√©gatif)
  )

```


```{r}
data_lsd <- dfm_lsd %>%
  dplyr::transmute(
    doc_id = doc_id,                 # Garde seulement l‚Äôidentifiant du document
    sentiment_global = net_perc     # Renomme le score net en "sentiment_global"
  )
  
```


```{r}
full_data_large <- cbind (full_data_large, data_lsd)
```



```{r fig.height=8, fig.width=15}

library(lubridate)

data_lsd_graph <- full_data_large %>%
  mutate(month = floor_date(date, unit = "month")) %>%  # Arrondit la date au d√©but du mois
  group_by(month) %>%                                   # Regroupe par mois
  summarise(sentiment = mean(sentiment_global, na.rm = TRUE))  # Moyenne mensuelle du score

# Graphique
ggplot(data_lsd_graph, aes(x = month, y = sentiment)) +
  geom_line(size = 1) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black", size = 1) +
  labs(x = NULL, y = "Score mensuel de sentiment") +
  theme_bw() +
  theme(
    axis.text.x = element_text(
      angle = 45, 
      size = 14, 
      vjust = 0.5,  # Ajuste la hauteur
      hjust = .5,
      color = "black"),
    axis.text.y = element_text(size = 14, color = "black"),
    axis.title.x = element_text(size = 18, color = "black"),
    axis.title.y = element_text(size = 18, color = "black"),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    legend.position = "none",
    strip.text = element_text(size = 18)
  ) +
  scale_y_continuous(expand = c(.05, 0)) +
  scale_x_date(
    limits = as.Date(c("1988-02-01", "2022-12-31")),
    date_breaks = "2 years",
    date_labels = "Jan %Y"
  )

```

```{r}
full_data_large <- full_data_large %>%
  
  # 1. Renommer les colonnes qui commencent par "X" en "Topic 1", "Topic 2", etc.
  rename_with(~ str_replace(., "^X", "Topic "), starts_with("X")) %>%
  
  # 2. Renommer la variable "sentiment_global" en "Sentiment global"
  rename(`Sentiment global` = sentiment_global) %>%
  
  # 3. Supprimer certaines colonnes inutiles
  dplyr::select(-doc_id, -day, -month, -ID)

```


######################### PARTIE 3 : REGRESSION ###############################################



```{r}
topics_regression <- full_data_large %>%
  # Regrouper les donn√©es par ann√©e et par source m√©diatique
  group_by(year, source) %>%
  
  # Calculer la moyenne de toutes les colonnes num√©riques pour chaque groupe
  summarise(
    across(
      where(is.numeric),              # S√©lectionner uniquement les colonnes num√©riques
      \(x) mean(x, na.rm = TRUE)      # Calculer la moyenne en ignorant les valeurs manquantes
    ),
    
    .groups = "drop"                  # Ne pas conserver les groupes apr√®s l'op√©ration
  )

```



## Importer les donn√©es agr√©g√©es administratives de statistique canada

```{r}
donnee_agrege_statcan <- read_xlsx("immigrant_chomage_homicide.xlsx")
```



```{r}
topics_regression <- merge(topics_regression, donnee_agrege_statcan)
```


```{r}
library(scales)  # Charge la librairie 'scales' pour utiliser la fonction rescale()

topics_regression <- topics_regression %>%
  mutate(
    # Appliquer une transformation centr√©e-r√©duite (min-max scaling) sur les variables continues s√©lectionn√©es
    across(
      c(pourcentage.immigrant, taux.chomage, nombre.homicide), 
      ~ rescale(.)  # Transformation des valeurs entre 0 et 1
    )
  )
```


```{r}
# Cr√©e une nouvelle variable 'Time' qui repr√©sente le nombre d'ann√©es √©coul√©es depuis 1988
topics_regression$Time <- topics_regression$year - 1988

```


```{r}
topics_regression  <- topics_regression  %>%
  mutate(
    # Attribue le parti politique au pouvoir en fonction des p√©riodes (approximativement les ann√©es de mandat en Ontario)
    parti = case_when(
      year %in% c(1988:1990, 2003:2013, 2013:2018) ~ "Liberal Party", 
      year %in% c(1990:1995) ~ "New Democratic Party",   
      year %in% c(1995:2002, 2002:2003) ~ "Progressive Conservative Party",
      year %in% c(2018:2022) ~ "Progressive Conservative Party"
    ),

    # Convertit la variable "parti" en facteur avec un ordre sp√©cifique (utile pour l'interpr√©tation des mod√®les)
    parti = factor(parti, levels = c("Liberal Party", 
                                     "Progressive Conservative Party",
                                     "New Democratic Party")),
    # Cr√©e des variables indicatrices pour les ann√©es √©lectorales (utiles dans les r√©gressions)
    year.election.1990 = ifelse(year == 1990, 1, 0),
    year.election.1995 = ifelse(year == 1995, 1, 0),
    year.election.1999 = ifelse(year == 1999, 1, 0),
    year.election.2003 = ifelse(year == 2003, 1, 0),
    year.election.2007 = ifelse(year == 2007, 1, 0),
    year.election.2011 = ifelse(year == 2011, 1, 0),
    year.election.2014 = ifelse(year == 2014, 1, 0),
    year.election.2018 = ifelse(year == 2018, 1, 0),
    year.election.2022 = ifelse(year == 2022, 1, 0))

```


                         
```{r}
## Liste des variables ind√©pendantes utilis√©es dans les r√©gressions
indep_var_list <- list(c(
  "Time",                     # Temps √©coul√© depuis 1988 (mesure temporelle)
  "pourcentage.immigrant",    # Pourcentage d'immigrants (variable contextuelle)
  "taux.chomage",             # Taux de ch√¥mage
  "parti",                    # Parti politique au pouvoir
  # Variables indicatrices pour les ann√©es √©lectorales (effets ponctuels)
  "year.election.1990", 
  "year.election.1995", 
  "year.election.1999", 
  "year.election.2003", 
  "year.election.2007", 
  "year.election.2011", 
  "year.election.2014",
  "year.election.2018", 
  "year.election.2022",
  "source",                   # Nom du m√©dia (ex. : Toronto Star, etc.)
  "nombre.homicide"           # Nombre d‚Äôhomicides (climat s√©curitaire)
))

# Noms des variables d√©pendantes √† analyser : les 14 topics + le sentiment global
dep_vars <- c(paste0("Topic ", 1:14), "Sentiment global")

# Ajoute des backticks autour des noms pour √©viter les erreurs si noms avec espace
dep_vars_bt <- paste0("`", dep_vars, "`")

```    


## Mod√®le lin√©aire mixte avec effet al√©atoire

```{r warning= FALSE, message=FALSE}


library(lme4)  # Pour le mod√®le √† effets mixtes

regression <- Map(function(x, y) {
  
  # Construire la formule dynamique : y ~ x1 + x2 + ... + (1 | year)
  
  fmla <- as.formula(paste(y, "~", paste(x, collapse = " + "), "+ (1|year)"))
  
  # Ajuster le mod√®le lin√©aire mixte avec effet al√©atoire sur l'ann√©e
  
  lmer(fmla, data = topics_regression, REML = TRUE)
}, indep_var_list, dep_vars_bt)

```


```{r}
# G√©n√©rer un tableau r√©capitulatif des r√©sultats de toutes les r√©gressions
# Avec :
# - show.ci = FALSE : ne pas afficher les intervalles de confiance
# - p.style = "stars" : afficher les niveaux de significativit√© avec des √©toiles
# - digits.p = 3 : arrondir les p-values √† 3 d√©cimales
# - digits = 2 : arrondir les coefficients √† 2 d√©cimales
# - file = "regression.doc" : exporter le tableau au format Word dans ce fichier

sjPlot::tab_model(regression, 
                  show.ci = FALSE, 
                  p.style = "stars", 
                  digits.p = 3, 
                  digits = 2,
                  file = "regression.doc")

```

############################################## FIN LABO  ##############################################


