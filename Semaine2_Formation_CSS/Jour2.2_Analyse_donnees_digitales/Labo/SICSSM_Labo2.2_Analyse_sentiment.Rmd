---
title: 'Labo 2.2: Analyse quantitative du texte'
subtitle: 'Analyse de sentiments'
author: "Visseho Adjiwanou, PhD."
date: "`r format(Sys.time(), '%d %B %Y')`"
output: html_document
---

# Analyse de sentiments

L'analyse de sentiment est une technique d'extraction d'information dans un corpus. Le but est de détecter les mots cherchés/voulus dans le corpus. Pour cela, on va se baser sur les dictionnaires qui existent ou définir notre propre dictionnaire.


```{r}

rm(list = ls())
#install.packages("tidytext")
#install.packages("textdata")

library(tidyverse)
library(tidytext)
library(textdata) # 
library(tm)
library(maps)
library(SnowballC)
library(wordcloud)


```

Nous allons charger les données de trump tweet


```{r}

trumptweets <- readRDS("../Données/trumptweets.RData")


tidy_trump_tweets <- 
  trumptweets %>% 
  select(created_at, text) %>% 
  unnest_tokens("word", text)     # Tokenise the data
  
tidy_trump_tweets  

head(tidy_trump_tweets, 12)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))

```

```{r}

data("stop_words")
# head(stopwords()) 
head(stop_words) 

tidy_trump_tweets <- 
  tidy_trump_tweets %>% 
  anti_join(stop_words)


head(tidy_trump_tweets, 20)

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))
  
```

```{r}

#tidy_trump_tweets <-
#  tidy_trump_tweets[-grep("https|t.co|amp|rt", tidy_trump_tweets$word), ] 

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("https|t.co|amp|rt", word))

tidy_trump_tweets %>% 
  count(word) %>% 
  arrange(desc(n))



# Enlever les chiffres

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  filter(!grepl("\\b\\d+\\b", word))


#tidy_trump_tweets <- tidy_trump_tweets[-grep("\\b\\d+\\b", tidy_trump_tweets$word),]

# Une fois de plus, tidytext transforme automatiquement tous les mots en minuscules.

# Enlever l'espace

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(word = gsub("\\s+","", word))

#tidy_trump_tweets$word <- gsub("\\s+","",tidy_trump_tweets$word)    

```

```{r}

tidy_trump_tweets %>% 
  count(word, sort = TRUE)


tidy_trump_tweets %>%          
  count(word) %>% 
  arrange(desc(n))

# Sélectionner les 20 mots les plus importants

top_20 <- 
  tidy_trump_tweets %>% 
  count(word, sort = TRUE) 

top_20 <- top_20[1:20, ]    
  
top_20  



```

```{r}

ggplot(top_20) +
  geom_bar(aes(x = word, y = n, fill = word), stat = "identity") +
  theme_minimal() +
  theme(axis.text = element_text(angle = 90, hjust = 1)) +
  labs(x = "mot", y = "Nombre de fois que le mot apparait dans un tweet") +
  guides(fill = FALSE)

```

```{r}


tidy_trump_tfidf <- 
  trumptweets %>%
  select(created_at, text) %>%
  unnest_tokens("word", text) %>%
 # anti_join(stop_words) %>%
  count(word, created_at) %>%
  bind_tf_idf(word, created_at, n)


top_tfidf <- tidy_trump_tfidf %>%
  arrange(desc(n))

top_tfidf

head(top_tfidf$word)

top_tfidf <- 
  tidy_trump_tfidf %>% 
  arrange(desc(tf_idf))

top_tfidf

```

## Notre propre dictionnaire

```{r}

#economic_dictionary <- c("economy","unemployment","trade","tariffs")

economic_dictionary <- "economy|unemployment|trade|tariffs|employment|tax"

economic_dictionary1 <- "Togo|Cameroon"


```

## Sélectionnons les tweets qui contiennent les mots relatifs à l'économie

```{r}

economic_tweets <- 
  trumptweets %>% 
  filter(str_detect(text, economic_dictionary))

head(economic_tweets$text)


```

## Analyse de sentiments avec les dictionnaires

```{r}

head(get_sentiments("afinn"), 24)

summary(get_sentiments("afinn"))

head(get_sentiments("bing"), 24)
summary(get_sentiments("bing"))

head(get_sentiments("nrc"), 24)
```

```{r}

dictionnaire <- get_sentiments("bing")

trump_tweet_sentiment <- 
  tidy_trump_tweets %>%
  inner_join(dictionnaire) %>%
  count(created_at, sentiment) 

               
head(trump_tweet_sentiment)

#head(tidy_trump_tweets)

ggplot(trump_tweet_sentiment) +
  geom_line(aes(x=created_at, y=n, color=sentiment), size=.5) +
  theme_minimal() +
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```

## On peut penser regrouper les dates pour avoir plusieurs données par jour

```{r}

library(lubridate)

tidy_trump_tweets <-
  tidy_trump_tweets %>% 
  mutate(date = date(created_at))


# Utilisons un meilleur regroupement

trump_tweet_sentiment1 <- 
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>%
  count(date, sentiment) 

ggplot(trump_tweet_sentiment1) +
  geom_line(aes(x=date, y=n, color=sentiment), size=.5) +
  theme_minimal() +
  labs(x = "Date", y = "Nombre de publications", title = "Evolution des sentiments") +
#  facet_wrap(~sentiment)+
  theme_bw()

```


## Tweet avec sentiments négatifs

```{r}

trump_sentiment_negatif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="negative") %>%
  count(date, sentiment)

trump_sentiment_negatif

trump_sentiment_positif <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  filter(sentiment=="positive") %>%
  count(date, sentiment)

trump_sentiment_positif

trump_sentiment <-
  tidy_trump_tweets %>%
  inner_join(get_sentiments("bing")) %>% 
  count(date, sentiment)

trump_sentiment 

```

```{r}

negatif <-
  ggplot(trump_sentiment_negatif) +
  geom_line(aes(x = date, y = n), color = "red") +
  labs(x = "Date", y = "Fréquence de mots négative dans les tweet de Trump")

negatif

```

## Est-ce que la fréquence de mots négatifs est associé à la popularité de Trump?

```{r}

trump_approval <- read.csv("https://projects.fivethirtyeight.com/trump-approval-data/approval_topline.csv")

head(trump_approval)

trump_approval <-
  trump_approval %>% 
  mutate(date = mdy(modeldate))

head(trump_approval)


approval_plot <-
  trump_approval %>%
  filter(subgroup == "Adults") %>%
  #filter(date > min(trump_sentiment_plot$date)) %>% 
  group_by(date) %>%
  summarise(approval = mean(approve_estimate))

head(approval_plot)

# Graphique

approval <-
  ggplot(approval_plot) +
  geom_line(aes(x = date, y = approval)) +
  #theme_minimal()+
  labs(x = "Date", y = "% des Américains qui aprouvent Trump")

approval

```

```{r}

library(ggpubr)

ggarrange(negatif, approval, nrow = 2)

```

## Analyse avec le dictionnaire NRC

```{r}

nrc <- get_sentiments("nrc")
nrc

trump_tweet_sentiment_nrc <-
  tidy_trump_tweets %>% 
  inner_join(nrc) %>% 
  count(date, sentiment)

trump_tweet_sentiment_nrc

# Roue des sentiments

tidy_trump_tweets_nrc1 <-
  tidy_trump_tweets %>%  
  inner_join(nrc) %>%
  group_by(sentiment) %>% 
  count() %>% 
  ungroup()%>% 
  arrange(desc(sentiment)) %>%
  mutate(percentage = round(n/sum(n), 4)*100,
         lab.pos = cumsum(percentage)-.5*percentage)


ggplot(data = tidy_trump_tweets_nrc1, aes(x = 2, y = percentage, fill = sentiment))+
  geom_bar(stat = "identity")+
  coord_polar("y", start = 200) +
  geom_text(aes(y = lab.pos, label = paste(percentage,"%", sep = "")), col = "white") +
  theme_void() +
  #scale_fill_brewer(palette = "Dark2")+
  xlim(.5, 2.5) +
  ggtitle("Sentiment dans les tweets de Trump")

```



Il existe de nombreux autres types d'analyse de sentiments, que nous n'avons pas le temps de couvrir ici. Cependant, vous devez savoir que différents outils d'analyse des sentiments fonctionnent mieux pour certains corpus que pour d'autres. Voici une figure d'un article récent qui applique une variété de dictionnaires de sentiments à différents corpus:

![](/Users/visseho/OneDrive - UQAM/Cours/Images_cours/comparaison.png)
Source: http://homepages.dcc.ufmg.br/~fabricio/download/cosn127-goncalves.pdf


